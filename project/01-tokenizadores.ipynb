{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:26:38.435402Z",
     "iopub.status.busy": "2024-06-30T02:26:38.435221Z",
     "iopub.status.idle": "2024-06-30T02:26:40.789915Z",
     "shell.execute_reply": "2024-06-30T02:26:40.789430Z",
     "shell.execute_reply.started": "2024-06-30T02:26:38.435387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.12.0\n",
      "Num GPUs Available:  1\n",
      "Device name: '/device:GPU:0'\n",
      "Device properties: {'compute_capability': (7, 5), 'device_name': 'NVIDIA GeForce RTX 2060 SUPER'}\n",
      "Não suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"Device name: '{device_name}'\")\n",
    "    print(\n",
    "        \"Device properties:\",\n",
    "        tf.config.experimental.get_device_details(\n",
    "            tf.config.experimental.list_physical_devices(\"GPU\")[0]\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    print(\"No GPU device found.\")\n",
    "\n",
    "# Check bfloat16 support\n",
    "bf16_supported = any(\n",
    "    \"bfloat16\"\n",
    "    in tf.config.experimental.get_device_details(gpu_device).get(\n",
    "        \"compute_capability\", \"\"\n",
    "    )\n",
    "    for gpu_device in tf.config.experimental.list_physical_devices(\"GPU\")\n",
    ")\n",
    "\n",
    "print(\"Suporta bfloat16.\" if bf16_supported else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL-LzAqpoGLC"
   },
   "source": [
    "# Tokenizadores - Introdução\n",
    "\n",
    "Na maioria das tarefas de PNL, o passo inicial na preparação dos seus dados é extrair um vocabulário de palavras do seu *corpus* (ou seja, textos de entrada). Você precisará definir como representar os textos em representações numéricas que podem ser usadas para treinar uma rede neural. Essas representações são chamadas de *tokens* e Tensorflow e Keras facilitam sua geração usando suas APIs. Você verá como fazer isso nas próximas células."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nt3uR9TPrUt"
   },
   "source": [
    "## Gerando o Vocabulário\n",
    "\n",
    "Neste notebook, você verá primeiro como pode fornecer um dicionário de consulta para cada palavra. O código abaixo pega uma lista de frases, depois pega cada palavra dessas frases e a atribui a um número inteiro. Isso é feito usando o método [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) e você pode obter o resultado olhando a propriedade `word_index`. <u>Palavras mais frequentes têm um índice menor</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:26:53.093824Z",
     "iopub.status.busy": "2024-06-30T02:26:53.093350Z",
     "iopub.status.idle": "2024-06-30T02:26:53.098122Z",
     "shell.execute_reply": "2024-06-30T02:26:53.097652Z",
     "shell.execute_reply.started": "2024-06-30T02:26:53.093806Z"
    },
    "id": "zaCMcjMQifQc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define input sentences\n",
    "sentences = [\"i love my dog\", \"I, love my cat\"]\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "\n",
    "# Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the indices and print it\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTPWesNaRdX2"
   },
   "source": [
    "O parâmetro `num_words` usado no inicializador especifica o número máximo de palavras menos uma (com base na frequência) a serem mantidas ao gerar sequências. Você verá isso em um exercício posterior. Por enquanto, o importante é notar que isso não afeta como o dicionário `word_index` é gerado. Você pode tentar passar `1` em vez de `100`, como mostrado na próxima célula, e obterá o mesmo `word_index`.\n",
    "\n",
    "Observe também que, por padrão, toda a pontuação é ignorada e as palavras são convertidas para minúsculas. Você pode substituir esses comportamentos modificando os argumentos `filters` e `lower` da classe `Tokenizer`, conforme descrito [aqui](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). Você pode tentar modificar esses argumentos na próxima célula abaixo e comparar o resultado com o gerado acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:26:55.043266Z",
     "iopub.status.busy": "2024-06-30T02:26:55.043014Z",
     "iopub.status.idle": "2024-06-30T02:26:55.046440Z",
     "shell.execute_reply": "2024-06-30T02:26:55.045922Z",
     "shell.execute_reply.started": "2024-06-30T02:26:55.043250Z"
    },
    "id": "VX1A1pDNoVKm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "# Define input sentences\n",
    "sentences = [\"i love my dog\", \"I, love my cat\", \"You love my dog!\"]\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=1)\n",
    "\n",
    "# Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the indices and print it\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Sequences\n",
    "\n",
    "No laboratório anterior, você viu como gerar um dicionário `word_index` para gerar tokens para cada palavra no seu corpus. Você pode então usar o resultado para converter cada uma das frases de entrada em uma sequência de tokens. Isso é feito utilizando o método [`texts_to_sequences()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) conforme mostrado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:29:24.991241Z",
     "iopub.status.busy": "2024-06-30T02:29:24.990978Z",
     "iopub.status.idle": "2024-06-30T02:29:24.994926Z",
     "shell.execute_reply": "2024-06-30T02:29:24.994527Z",
     "shell.execute_reply.started": "2024-06-30T02:29:24.991225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define your input texts\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "\n",
    "# Tokenize the input sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index dictionary\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate list of token sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Conforme mencionado no vídeo, você geralmente precisará preencher (pad) as sequências para um comprimento uniforme, pois é isso que o seu modelo espera. Você pode usar o método [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) para isso. Por padrão, ele preencherá de acordo com o comprimento da sequência mais longa. Você pode substituir isso com o argumento `maxlen` para definir um comprimento específico. Sinta-se à vontade para experimentar [outros argumentos](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences#args) mostrados em aula e comparar o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:31:23.408434Z",
     "iopub.status.busy": "2024-06-30T02:31:23.408235Z",
     "iopub.status.idle": "2024-06-30T02:31:23.416759Z",
     "shell.execute_reply": "2024-06-30T02:31:23.416329Z",
     "shell.execute_reply.started": "2024-06-30T02:31:23.408419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to a uniform length\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens fora do vocabulário\n",
    "\n",
    "Observe que você definiu um `oov_token` quando o `Tokenizer` foi inicializado anteriormente. Isso será usado quando você tiver palavras de entrada que não são encontradas no dicionário `word_index`. Por exemplo, você pode decidir coletar mais texto após o treinamento inicial e optar por não regenerar o `word_index`. Você verá isso em ação na célula abaixo. Note que o token `1` é inserido para palavras que não são encontradas no dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:32:44.913178Z",
     "iopub.status.busy": "2024-06-30T02:32:44.912935Z",
     "iopub.status.idle": "2024-06-30T02:32:44.916415Z",
     "shell.execute_reply": "2024-06-30T02:32:44.916028Z",
     "shell.execute_reply.started": "2024-06-30T02:32:44.913163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "# Generate the sequences\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Print the word index dictionary\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "\n",
    "# Print the sequences with OOV\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "# Print the padded result\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab: Tokenizing the Sarcasm Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste laboratório, você aplicará o que aprendeu nos dois exercícios anteriores para pré-processar o [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home). Este dataset contém manchetes de notícias que são rotuladas como sarcásticas ou não. Você revisitará este dataset em laboratórios posteriores, por isso é bom se familiarizar com ele agora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixar e inspecionar o dataset\n",
    "\n",
    "Primeiro, você irá buscar o dataset e visualizar alguns de seus elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:35:18.902020Z",
     "iopub.status.busy": "2024-06-30T02:35:18.901661Z",
     "iopub.status.idle": "2024-06-30T02:35:21.276881Z",
     "shell.execute_reply": "2024-06-30T02:35:21.276365Z",
     "shell.execute_reply.started": "2024-06-30T02:35:18.901992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-30 02:35:19--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.219.155, 142.251.132.59, 172.217.30.59, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.219.155|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: ‘sarcasm.json’\n",
      "\n",
      "sarcasm.json        100%[===================>]   5.38M  3.43MB/s    in 1.6s    \n",
      "\n",
      "2024-06-30 02:35:21 (3.43 MB/s) - ‘sarcasm.json’ saved [5643545/5643545]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset está salvo como um arquivo [JSON](https://www.json.org/json-en.html) e você pode usar o módulo [`json`](https://docs.python.org/3/library/json.html) do Python para carregá-lo em seu ambiente de trabalho. A célula abaixo desempacota o arquivo JSON em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:36:21.539728Z",
     "iopub.status.busy": "2024-06-30T02:36:21.539454Z",
     "iopub.status.idle": "2024-06-30T02:36:21.569340Z",
     "shell.execute_reply": "2024-06-30T02:36:21.568901Z",
     "shell.execute_reply.started": "2024-06-30T02:36:21.539712Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"./sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode inspecionar alguns dos elementos na lista. Você notará que cada elemento consiste em um dicionário com um link de URL, a manchete real e um rótulo chamado `is_sarcastic`. Abaixo estão impressos dois elementos com rótulos contrastantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:37:03.966356Z",
     "iopub.status.busy": "2024-06-30T02:37:03.966027Z",
     "iopub.status.idle": "2024-06-30T02:37:03.969078Z",
     "shell.execute_reply": "2024-06-30T02:37:03.968721Z",
     "shell.execute_reply.started": "2024-06-30T02:37:03.966334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n",
      "{'article_link': 'https://www.theonion.com/pediatricians-announce-2011-newborns-are-ugliest-babies-1819572977', 'headline': 'pediatricians announce 2011 newborns are ugliest babies in 30 years', 'is_sarcastic': 1}\n"
     ]
    }
   ],
   "source": [
    "# Non-sarcastic headline\n",
    "print(datastore[0])\n",
    "\n",
    "# Sarcastic headline\n",
    "print(datastore[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, você pode coletar todas as URLs, manchetes e rótulos para facilitar o processamento ao usar o tokenizer. Para este laboratório, você precisará apenas das manchetes, mas incluímos o código para coletar as URLs e os rótulos também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:37:39.437528Z",
     "iopub.status.busy": "2024-06-30T02:37:39.437250Z",
     "iopub.status.idle": "2024-06-30T02:37:39.446209Z",
     "shell.execute_reply": "2024-06-30T02:37:39.445634Z",
     "shell.execute_reply.started": "2024-06-30T02:37:39.437506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "# Append elements in the dictionaries into each list\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento das manchetes\n",
    "\n",
    "Você pode converter a lista de `sentences` acima em sequências preenchidas (padded) usando os mesmos métodos que você vem utilizando nos exercícios anteriores. A célula abaixo gera o dicionário `word_index` e gera a lista de sequências preenchidas para cada uma das 26.709 manchetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:38:35.607587Z",
     "iopub.status.busy": "2024-06-30T02:38:35.607303Z",
     "iopub.status.idle": "2024-06-30T02:38:35.612513Z",
     "shell.execute_reply": "2024-06-30T02:38:35.612035Z",
     "shell.execute_reply.started": "2024-06-30T02:38:35.607570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T02:40:46.487139Z",
     "iopub.status.busy": "2024-06-30T02:40:46.486884Z",
     "iopub.status.idle": "2024-06-30T02:40:46.973768Z",
     "shell.execute_reply": "2024-06-30T02:40:46.973204Z",
     "shell.execute_reply.started": "2024-06-30T02:40:46.487121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in word_index: 29657\n",
      "\n",
      "sample headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "padded sequence: [  145   838     2   907  1749  2093   582  4719   221   143    39    46\n",
      "     2 10736     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "shape of padded sequences: (26709, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Print the length of the word index\n",
    "word_index = tokenizer.word_index\n",
    "print(f'number of words in word_index: {len(word_index)}')\n",
    "\n",
    "# Print the word index\n",
    "# print(f'word_index: {word_index}')\n",
    "print()\n",
    "\n",
    "# Generate and pad the sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Print a sample headline\n",
    "index = 2\n",
    "print(f'sample headline: {sentences[index]}')\n",
    "print(f'padded sequence: {padded[index]}')\n",
    "print()\n",
    "\n",
    "# Print dimensions of padded sequences\n",
    "print(f'shape of padded sequences: {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W1_Lab_1_tokenize_basic.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb",
     "timestamp": 1642431620601
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
